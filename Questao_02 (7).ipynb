{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 p style='text-align: center;'>Base Dados ANTAQ <h1>\n",
    "<h2 p style='text-align: center;'> Gerção das Tabelas fatos: atracacao_fato e carga_fato <h2>\n",
    "<h3 p style='text-align: center;'>Extração e transformação dos dados em Dataframes<h3>\n",
    "<h3 p style='text-align: center;'>Questão 02<h3>\n",
    "<h4 p style='text-align: center;'> Autor: Elifranio Alves Cruz <h4>\n",
    "<h2 p style='text-align: left;'> Extração <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definições das bibliotecas\n",
    "import pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Collinear Points\").setMaster(\"local[4]\") # inicializar o Spark unsando o contexto de 4 núcleo de trabalhos (jobs) \n",
    "#sc = SparkContext(conf=conf)    \n",
    "from pyspark.rdd import RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-E9I1QDLM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accumulator',\n",
       " 'AccumulatorParam',\n",
       " 'BarrierTaskContext',\n",
       " 'BarrierTaskInfo',\n",
       " 'BasicProfiler',\n",
       " 'Broadcast',\n",
       " 'HiveContext',\n",
       " 'InheritableThread',\n",
       " 'MarshalSerializer',\n",
       " 'PickleSerializer',\n",
       " 'Profiler',\n",
       " 'RDD',\n",
       " 'RDDBarrier',\n",
       " 'Row',\n",
       " 'SQLContext',\n",
       " 'SparkConf',\n",
       " 'SparkContext',\n",
       " 'SparkFiles',\n",
       " 'SparkJobInfo',\n",
       " 'SparkStageInfo',\n",
       " 'StatusTracker',\n",
       " 'StorageLevel',\n",
       " 'TaskContext',\n",
       " '_NoValue',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_globals',\n",
       " 'accumulators',\n",
       " 'broadcast',\n",
       " 'cloudpickle',\n",
       " 'conf',\n",
       " 'context',\n",
       " 'copy_func',\n",
       " 'files',\n",
       " 'find_spark_home',\n",
       " 'java_gateway',\n",
       " 'join',\n",
       " 'keyword_only',\n",
       " 'profiler',\n",
       " 'rdd',\n",
       " 'rddsampler',\n",
       " 'resource',\n",
       " 'resultiterable',\n",
       " 'serializers',\n",
       " 'shuffle',\n",
       " 'since',\n",
       " 'sql',\n",
       " 'statcounter',\n",
       " 'status',\n",
       " 'storagelevel',\n",
       " 'taskcontext',\n",
       " 'traceback_utils',\n",
       " 'types',\n",
       " 'util',\n",
       " 'version',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma sessão usanda para DF ( antaq) exemplo\n",
    "spark = SparkSession.builder.appName(\"ANTAQ\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pode-se alternar 2018, 2019 e 2020 de forma manual de acordo com os arquivos nos diretórios, neste caso para \n",
    "# a tabela de acordos bilaterais.Ex.  ano = 2018; ou ano = 2019 ou ano = 2020\n",
    "ano = \"2020\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano+\"/\"+ano+\"AcordosBilaterais.txt\"):\n",
    "        df = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano+\"/\"+ano+\"AcordosBilaterais.txt\", header=True, sep=';')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "|NacionalidadeEmbarcacao|Ano Acordo Bilateral|Total Acordo Bilateral|AcordoTipoNavegacao|     País|FlagEmbarqueDesembarque|\n",
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "|          Não Informado|                2010|       1906211,1270003|        Longo Curso|ARGENTINA|                      1|\n",
      "|                   null|                2010|       3160823,6960002|        Longo Curso|ARGENTINA|                      1|\n",
      "|           Estrangeiras|                2010|       2158177,1970002|        Longo Curso|ARGENTINA|                      1|\n",
      "|           Brasilieiras|                2010|       395849,45900001|        Longo Curso|ARGENTINA|                      1|\n",
      "|                   null|                2010|               3011,25|           Interior|ARGENTINA|                      1|\n",
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(NacionalidadeEmbarcacao='Não Informado', Ano Acordo Bilateral='2010', Total Acordo Bilateral='1906211,1270003', AcordoTipoNavegacao='Longo Curso', País='ARGENTINA', FlagEmbarqueDesembarque='1')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pegar a primeira linha\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(NacionalidadeEmbarcacao='Não Informado', Ano Acordo Bilateral='2010', Total Acordo Bilateral='1906211,1270003', AcordoTipoNavegacao='Longo Curso', País='ARGENTINA', FlagEmbarqueDesembarque='1'),\n",
       " Row(NacionalidadeEmbarcacao=None, Ano Acordo Bilateral='2010', Total Acordo Bilateral='3160823,6960002', AcordoTipoNavegacao='Longo Curso', País='ARGENTINA', FlagEmbarqueDesembarque='1'),\n",
       " Row(NacionalidadeEmbarcacao='Estrangeiras', Ano Acordo Bilateral='2010', Total Acordo Bilateral='2158177,1970002', AcordoTipoNavegacao='Longo Curso', País='ARGENTINA', FlagEmbarqueDesembarque='1'),\n",
       " Row(NacionalidadeEmbarcacao='Brasilieiras', Ano Acordo Bilateral='2010', Total Acordo Bilateral='395849,45900001', AcordoTipoNavegacao='Longo Curso', País='ARGENTINA', FlagEmbarqueDesembarque='1'),\n",
       " Row(NacionalidadeEmbarcacao=None, Ano Acordo Bilateral='2010', Total Acordo Bilateral='3011,25', AcordoTipoNavegacao='Interior', País='ARGENTINA', FlagEmbarqueDesembarque='1')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pegar 5 cabeçalhos\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NacionalidadeEmbarcacao',\n",
       " 'Ano Acordo Bilateral',\n",
       " 'Total Acordo Bilateral',\n",
       " 'AcordoTipoNavegacao',\n",
       " 'País',\n",
       " 'FlagEmbarqueDesembarque']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checar o nome das colunas\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NacionalidadeEmbarcacao', 'string'),\n",
       " ('Ano Acordo Bilateral', 'string'),\n",
       " ('Total Acordo Bilateral', 'string'),\n",
       " ('AcordoTipoNavegacao', 'string'),\n",
       " ('País', 'string'),\n",
       " ('FlagEmbarqueDesembarque', 'string')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checar os tipos de dados das colunas\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NacionalidadeEmbarcacao: string (nullable = true)\n",
      " |-- Ano Acordo Bilateral: string (nullable = true)\n",
      " |-- Total Acordo Bilateral: string (nullable = true)\n",
      " |-- AcordoTipoNavegacao: string (nullable = true)\n",
      " |-- País: string (nullable = true)\n",
      " |-- FlagEmbarqueDesembarque: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pegarv o esquema do dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificar o número de linhas\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checar o número de colunas\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 6\n"
     ]
    }
   ],
   "source": [
    "# checar a forma (linhas, colunas)\n",
    "print(df.count(), len(df.columns) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tabela Acordo Bilateral######\n",
    "#Pode-se alternar 2018, 2019 e 2020 de forma manual de acordo com os arquivos nos diretórios, neste caso para \n",
    "# a tabela de acordos bilaterais.Ex.  ano = 2018; ou ano = 2019 ou ano = 2020\n",
    "#Extraindo de forma manual no modelo de I/O do pyspark os dados da tabela de Acordo Bilateral\n",
    "\n",
    "# criar uma sessão usanda para extração da tabela Acordo Bilateral\n",
    "spark = SparkSession.builder.appName(\"ANTAQ.tb_ab\").getOrCreate()\n",
    "\n",
    "\n",
    "ano2020 = \"2020\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"AcordosBilaterais.txt\"):\n",
    "        tb_ab2020 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"AcordosBilaterais.txt\", header=True, sep=';')\n",
    "            \n",
    "ano2019 = \"2019\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"AcordosBilaterais.txt\"):\n",
    "        tb_ab2019 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"AcordosBilaterais.txt\", header=True, sep=';')\n",
    "        \n",
    "ano2018 = \"2018\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"AcordosBilaterais.txt\"):\n",
    "        tb_ab2018 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"AcordosBilaterais.txt\", header=True, sep=';')   \n",
    "        \n",
    "tb_ab = tb_ab2020.union(tb_ab2019.union(tb_ab2018))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "|NacionalidadeEmbarcacao|Ano Acordo Bilateral|Total Acordo Bilateral|AcordoTipoNavegacao|     País|FlagEmbarqueDesembarque|\n",
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "|          Não Informado|                2010|       1906211,1270003|        Longo Curso|ARGENTINA|                      1|\n",
      "|                   null|                2010|       3160823,6960002|        Longo Curso|ARGENTINA|                      1|\n",
      "|           Estrangeiras|                2010|       2158177,1970002|        Longo Curso|ARGENTINA|                      1|\n",
      "|           Brasilieiras|                2010|       395849,45900001|        Longo Curso|ARGENTINA|                      1|\n",
      "|                   null|                2010|               3011,25|           Interior|ARGENTINA|                      1|\n",
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_ab2020.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "|NacionalidadeEmbarcacao|Ano Acordo Bilateral|Total Acordo Bilateral|AcordoTipoNavegacao|     País|FlagEmbarqueDesembarque|\n",
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "|          Não Informado|                2010|       1906211,1270003|        Longo Curso|ARGENTINA|                      1|\n",
      "|                   null|                2010|       3160823,6960002|        Longo Curso|ARGENTINA|                      1|\n",
      "|           Estrangeiras|                2010|       2158177,1970002|        Longo Curso|ARGENTINA|                      1|\n",
      "|           Brasilieiras|                2010|       395849,45900001|        Longo Curso|ARGENTINA|                      1|\n",
      "|                   null|                2010|               3011,25|           Interior|ARGENTINA|                      1|\n",
      "+-----------------------+--------------------+----------------------+-------------------+---------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_ab.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_ab2020.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_ab2019.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tb_ab2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2243"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_ab.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tabela Atracação######\n",
    "#Pode-se alternar 2018, 2019 e 2020 de forma manual de acordo com os arquivos nos diretórios, neste caso para \n",
    "# a tabela de atracação.Ex.  ano = 2018; ou ano = 2019 ou ano = 2020\n",
    "#Extraindo de forma manual no modelo de I/O do pyspark os dados da tabela de Atracação\n",
    "\n",
    "# criar uma sessão usanda para extração da tabela Atracacao\n",
    "spark = SparkSession.builder.appName(\"ANTAQ.tb_atr\").getOrCreate()\n",
    "\n",
    "ano2020 = \"2020\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"Atracacao.txt\"):\n",
    "        tb_atr2020 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"Atracacao.txt\", header=True, sep=';')\n",
    "            \n",
    "ano2019 = \"2019\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"Atracacao.txt\"):\n",
    "        tb_atr2019 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"Atracacao.txt\", header=True, sep=';')\n",
    "        \n",
    "ano2018 = \"2018\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"Atracacao.txt\"):\n",
    "        tb_atr2018 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"Atracacao.txt\", header=True, sep=';')   \n",
    "        \n",
    "tb_atr = tb_atr2020.union(tb_atr2019.union(tb_atr2018))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDAtracacao='2462', CDTUP='BR', IDBerco=None, Berço=None, Porto Atracação='n/a', Apelido Instalação Portuária='n/a', Complexo Portuário='Não Classificado', Tipo da Autoridade Portuária='Porto Público', Data Atracação=None, Data Chegada=None, Data Desatracação='01/09/2018 00:00:00', Data Início Operação=None, Data Término Operação=None, Ano='2018', Mes='set', Tipo de Operação=None, Tipo de Navegação da Atracação=None, Nacionalidade do Armador=None, FlagMCOperacaoAtracacao='1', Terminal=None, Município=None, UF=None, SGUF=None, Região Geográfica=None, Nº da Capitania=None, Nº do IMO=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr2018.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDAtracacao='2462', CDTUP='BR', IDBerco=None, Berço=None, Porto Atracação='n/a', Apelido Instalação Portuária='n/a', Complexo Portuário='Não Classificado', Tipo da Autoridade Portuária='Porto Público', Data Atracação=None, Data Chegada=None, Data Desatracação='01/09/2018 00:00:00', Data Início Operação=None, Data Término Operação=None, Ano='2018', Mes='set', Tipo de Operação=None, Tipo de Navegação da Atracação=None, Nacionalidade do Armador=None, FlagMCOperacaoAtracacao='1', Terminal=None, Município=None, UF=None, SGUF=None, Região Geográfica=None, Nº da Capitania=None, Nº do IMO=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+-----+---------------+----------------------------+------------------+----------------------------+--------------+------------+-------------------+--------------------+---------------------+----+---+----------------+------------------------------+------------------------+-----------------------+--------+---------+----+----+-----------------+---------------+---------+\n",
      "|IDAtracacao|CDTUP|IDBerco|Berço|Porto Atracação|Apelido Instalação Portuária|Complexo Portuário|Tipo da Autoridade Portuária|Data Atracação|Data Chegada|  Data Desatracação|Data Início Operação|Data Término Operação| Ano|Mes|Tipo de Operação|Tipo de Navegação da Atracação|Nacionalidade do Armador|FlagMCOperacaoAtracacao|Terminal|Município|  UF|SGUF|Região Geográfica|Nº da Capitania|Nº do IMO|\n",
      "+-----------+-----+-------+-----+---------------+----------------------------+------------------+----------------------------+--------------+------------+-------------------+--------------------+---------------------+----+---+----------------+------------------------------+------------------------+-----------------------+--------+---------+----+----+-----------------+---------------+---------+\n",
      "|       2462|   BR|   null| null|            n/a|                         n/a|  Não Classificado|               Porto Público|          null|        null|01/09/2018 00:00:00|                null|                 null|2018|set|            null|                          null|                    null|                      1|    null|     null|null|null|             null|           null|     null|\n",
      "|       2487|   BR|   null| null|            n/a|                         n/a|  Não Classificado|               Porto Público|          null|        null|01/10/2018 00:00:00|                null|                 null|2018|out|            null|                          null|                    null|                      1|    null|     null|null|null|             null|           null|     null|\n",
      "|       2474|   BR|   null| null|            n/a|                         n/a|  Não Classificado|               Porto Público|          null|        null|01/10/2018 00:00:00|                null|                 null|2018|out|            null|                          null|                    null|                      1|    null|     null|null|null|             null|           null|     null|\n",
      "|       2489|   BR|   null| null|            n/a|                         n/a|  Não Classificado|               Porto Público|          null|        null|01/10/2018 00:00:00|                null|                 null|2018|out|            null|                          null|                    null|                      1|    null|     null|null|null|             null|           null|     null|\n",
      "|       2488|   BR|   null| null|            n/a|                         n/a|  Não Classificado|               Porto Público|          null|        null|01/10/2018 00:00:00|                null|                 null|2018|out|            null|                          null|                    null|                      1|    null|     null|null|null|             null|           null|     null|\n",
      "+-----------+-----+-------+-----+---------------+----------------------------+------------------+----------------------------+--------------+------------+-------------------+--------------------+---------------------+----+---+----------------+------------------------------+------------------------+-----------------------+--------+---------+----+----+-----------------+---------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_atr2018.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76284"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr2020.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77208"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr2019.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76082"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229574"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tabela Carga######\n",
    "#Pode-se alternar 2018, 2019 e 2020 de forma manual de acordo com os arquivos nos diretórios, neste caso para \n",
    "# a tabela de Carga.Ex.  ano = 2018; ou ano = 2019 ou ano = 2020\n",
    "#Extraindo de forma manual no modelo de I/O do pyspark os dados da tabela de Carga\n",
    "\n",
    "# criar uma sessão usanda para extração da tabela Carga\n",
    "spark = SparkSession.builder.appName(\"ANTAQ.tb_carga\").getOrCreate()\n",
    "\n",
    "ano2020 = \"2020\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"Carga.txt\"):\n",
    "        tb_carga2020 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"Carga.txt\", header=True, sep=';')\n",
    "            \n",
    "ano2019 = \"2019\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"Carga.txt\"):\n",
    "        tb_carga2019 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"Carga.txt\", header=True, sep=';')\n",
    "        \n",
    "ano2018 = \"2018\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"Carga.txt\"):\n",
    "        tb_carga2018 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"Carga.txt\", header=True, sep=';')   \n",
    "        \n",
    "tb_carga = tb_carga2020.union(tb_carga2019.union(tb_carga2018))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDCarga='25376365', IDAtracacao='1048499', Origem='BR200', Destino='BRBEL', CDMercadoria='2710', Tipo Operação da Carga='Interior', Carga Geral Acondicionamento=None, ConteinerEstado=None, Tipo Navegação='Interior', FlagAutorizacao='S', FlagCabotagem='0', FlagCabotagemMovimentacao='0', FlagConteinerTamanho=None, FlagLongoCurso='0', FlagMCOperacaoCarga='1', FlagOffshore='0', FlagTransporteViaInterioir='1', Percurso Transporte em vias Interiores='Interior de percurso não identificado', Percurso Transporte Interiores='Navegação Interior', STNaturezaCarga='Exclusivo', STSH2='Exclusivo', STSH4='Exclusivo', Natureza da Carga='Granel Líquido e Gasoso', Sentido='Desembarcados', TEU='0', QTCarga='0', VLPesoCargaBruta='1005,32')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga2020.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+-------+--------------+\n",
      "| IDCarga|IDAtracacao| Origem|Destino|Tipo Navegação|\n",
      "+--------+-----------+-------+-------+--------------+\n",
      "|25376365|    1048499|  BR200|  BRBEL|      Interior|\n",
      "|25377972|    1048717|  USMOB|BRRJ016|   Longo Curso|\n",
      "|25378091|    1048758|BRRS008|BRRS003|      Interior|\n",
      "|25381355|    1048861|  BRBEL|  BRVDC|      Interior|\n",
      "|25381441|    1048922|BRPA001|  IEAUG|   Longo Curso|\n",
      "+--------+-----------+-------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_carga2020.select('IDCarga','IDAtracacao', 'Origem','Destino','Tipo Navegação').show(5) #Fica fora do escopo devido a quantidade colunas ser maior que a capacidade de resolução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096101"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga2020.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018617"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga2019.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2087525"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6202243"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tabela Carga_Conteinerizada ######\n",
    "#Pode-se alternar 2018, 2019 e 2020 de forma manual de acordo com os arquivos nos diretórios, neste caso para \n",
    "# a tabela de Carga_Conteinerizada.Ex.  ano = 2018; ou ano = 2019 ou ano = 2020\n",
    "#Extraindo de forma manual no modelo de I/O do pyspark os dados da tabela de Carga_Conteinerizada\n",
    "\n",
    "# criar uma sessão usanda para extração da tabela Carga_Conteinerizada\n",
    "spark = SparkSession.builder.appName(\"ANTAQ.tb_carga_contz\").getOrCreate()\n",
    "\n",
    "ano2020 = \"2020\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"Carga_Conteinerizada.txt\"):\n",
    "        tb_carga_contz2020 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"Carga_Conteinerizada.txt\", header=True, sep=';')\n",
    "            \n",
    "ano2019 = \"2019\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"Carga_Conteinerizada.txt\"):\n",
    "        tb_carga_contz2019 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"Carga_Conteinerizada.txt\", header=True, sep=';')\n",
    "        \n",
    "ano2018 = \"2018\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"Carga_Conteinerizada.txt\"):\n",
    "        tb_carga_contz2018 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"Carga_Conteinerizada.txt\", header=True, sep=';')   \n",
    "        \n",
    "tb_carga_contz = tb_carga_contz2020.union(tb_carga_contz2019.union(tb_carga_contz2018)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-------------------------+\n",
      "| IDCarga|CDMercadoriaConteinerizada|VLPesoCargaConteinerizada|\n",
      "+--------+--------------------------+-------------------------+\n",
      "|25506812|                      0101|                     17,8|\n",
      "|25506832|                      0101|                    14,12|\n",
      "|25507093|                      0101|                     17,8|\n",
      "|25507107|                      0101|                    36,44|\n",
      "|25507177|                      0101|                     0,92|\n",
      "+--------+--------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_carga_contz2020.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-------------------------+\n",
      "| IDCarga|CDMercadoriaConteinerizada|VLPesoCargaConteinerizada|\n",
      "+--------+--------------------------+-------------------------+\n",
      "|25506812|                      0101|                     17,8|\n",
      "|25506832|                      0101|                    14,12|\n",
      "|25507093|                      0101|                     17,8|\n",
      "|25507107|                      0101|                    36,44|\n",
      "|25507177|                      0101|                     0,92|\n",
      "+--------+--------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_carga_contz.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3570103"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_contz2020.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3500499"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_contz2019.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3651101"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_contz2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10721703"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_contz.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tabela Carga_Regiao ######\n",
    "#Pode-se alternar 2018, 2019 e 2020 de forma manual de acordo com os arquivos nos diretórios, neste caso para \n",
    "# a tabela de Carga_Regiao.Ex.  ano = 2018; ou ano = 2019 ou ano = 2020\n",
    "#Extraindo de forma manual no modelo de I/O do pyspark os dados da tabela de Carga_Regiao\n",
    "\n",
    "# criar uma sessão usanda para extração da tabela Carga_Regiao\n",
    "spark = SparkSession.builder.appName(\"ANTAQ.tb_carga_reg\").getOrCreate()\n",
    "\n",
    "ano2020 = \"2020\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"Carga_Regiao.txt\"):\n",
    "        tb_carga_reg2020 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"Carga_Regiao.txt\", header=True, sep=';')\n",
    "            \n",
    "ano2019 = \"2019\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"Carga_Regiao.txt\"):\n",
    "        tb_carga_reg2019 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"Carga_Regiao.txt\", header=True, sep=';')\n",
    "        \n",
    "ano2018 = \"2018\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"Carga_Regiao.txt\"):\n",
    "        tb_carga_reg2018 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"Carga_Regiao.txt\", header=True, sep=';')   \n",
    "        \n",
    "tb_carga_reg = tb_carga_reg2020.union(tb_carga_reg2019.union(tb_carga_reg2018)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+\n",
      "| IDCarga| Região Hidrográfica|ValorMovimentado|\n",
      "+--------+--------------------+----------------+\n",
      "|25376365|                null|         1005,32|\n",
      "|25381441|Região Hidrográfi...|           55000|\n",
      "|25381442|Região Hidrográfi...|         1334,24|\n",
      "|25381556|Região Hidrográfi...|         1403,95|\n",
      "|25381562|Região Hidrográfi...|         1686,19|\n",
      "+--------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_carga_reg2020.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+\n",
      "| IDCarga| Região Hidrográfica|ValorMovimentado|\n",
      "+--------+--------------------+----------------+\n",
      "|25376365|                null|         1005,32|\n",
      "|25381441|Região Hidrográfi...|           55000|\n",
      "|25381442|Região Hidrográfi...|         1334,24|\n",
      "|25381556|Região Hidrográfi...|         1403,95|\n",
      "|25381562|Região Hidrográfi...|         1686,19|\n",
      "+--------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_carga_reg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506791"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_reg2020.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482857"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_reg2019.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563870"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_reg2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1553518"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_reg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tabela TemposAtracacao ######\n",
    "#Pode-se alternar 2018, 2019 e 2020 de forma manual de acordo com os arquivos nos diretórios, neste caso para \n",
    "# a tabela de TemposAtracacao.Ex.  ano = 2018; ou ano = 2019 ou ano = 2020\n",
    "#Extraindo de forma manual no modelo de I/O do pyspark os dados da tabela de TemposAtracacao\n",
    "\n",
    "# criar uma sessão usanda para extração da tabela TemposAtracacao\n",
    "spark = SparkSession.builder.appName(\"ANTAQ.tb_temp_atr\").getOrCreate()\n",
    "\n",
    "ano2020 = \"2020\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"TemposAtracacao.txt\"):\n",
    "        tb_temp_atr2020 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2020+\"/\"+ano2020+\"TemposAtracacao.txt\", header=True, sep=';')\n",
    "            \n",
    "ano2019 = \"2019\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"TemposAtracacao.txt\"):\n",
    "        tb_temp_atr2019 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2019+\"/\"+ano2019+\"TemposAtracacao.txt\", header=True, sep=';')\n",
    "        \n",
    "ano2018 = \"2018\"\n",
    "   # verifica se o arquivo existe no dietório\n",
    "if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"TemposAtracacao.txt\"):\n",
    "        tb_temp_atr2018 = spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+ano2018+\"/\"+ano2018+\"TemposAtracacao.txt\", header=True, sep=';')   \n",
    "        \n",
    "tb_temp_atr = tb_temp_atr2020.union(tb_temp_atr2019.union(tb_temp_atr2018))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+----------------+---------------+-------------------+---------------+---------------+\n",
      "|IDAtracacao|TEsperaAtracacao| TEsperaInicioOp|      TOperacao|TEsperaDesatracacao|      TAtracado|       TEstadia|\n",
      "+-----------+----------------+----------------+---------------+-------------------+---------------+---------------+\n",
      "|    1117343|0,48333333333721|0,18333333340706|39,249999999942|   0,25000000011642|39,683333333465|40,166666666802|\n",
      "|    1117350|0,48333333333721|0,18333333323244|147,08333333343|   0,24999999994179|              0|147,99999999994|\n",
      "|    1117358|0,48333333333721|0,18333333340706|13,249999999884|   0,25000000011642|13,683333333407|14,166666666744|\n",
      "|    1117341|0,48333333333721|0,18333333323244|          18,75|   0,25000000011642|19,183333333349|19,666666666686|\n",
      "|    1117352|0,48333333333721|0,18333333340706|15,999999999942|   0,24999999994179|16,433333333291|16,916666666628|\n",
      "+-----------+----------------+----------------+---------------+-------------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_temp_atr2020.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+----------------+---------------+-------------------+---------------+---------------+\n",
      "|IDAtracacao|TEsperaAtracacao| TEsperaInicioOp|      TOperacao|TEsperaDesatracacao|      TAtracado|       TEstadia|\n",
      "+-----------+----------------+----------------+---------------+-------------------+---------------+---------------+\n",
      "|    1117343|0,48333333333721|0,18333333340706|39,249999999942|   0,25000000011642|39,683333333465|40,166666666802|\n",
      "|    1117350|0,48333333333721|0,18333333323244|147,08333333343|   0,24999999994179|              0|147,99999999994|\n",
      "|    1117358|0,48333333333721|0,18333333340706|13,249999999884|   0,25000000011642|13,683333333407|14,166666666744|\n",
      "|    1117341|0,48333333333721|0,18333333323244|          18,75|   0,25000000011642|19,183333333349|19,666666666686|\n",
      "|    1117352|0,48333333333721|0,18333333340706|15,999999999942|   0,24999999994179|16,433333333291|16,916666666628|\n",
      "+-----------+----------------+----------------+---------------+-------------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_temp_atr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74941"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_temp_atr2020.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76536"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_temp_atr2019.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75642"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_temp_atr2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227119"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_temp_atr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transformação <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário das tabelas Extraidas\n",
    "#tb_ab  ; tb_atr ; tb_carga ; tb_carga_contz; tb_carga_reg; tb_temp_atr\n",
    "###############\n",
    "# tb_ab - Tabela Acordos Bilaterais\n",
    "# tb_atr - Tabela Atracacao\n",
    "# tb_carga - Tabela Carga\n",
    "# tb_carga_contz - Tabela Carga_Conteinerizada\n",
    "# tb_carga_reg - Tabela Carga_Regiao\n",
    "# tb_temp_atr - Tabela TemposAtracacao\n",
    "###############\n",
    "\n",
    "# Dicionário dos atributos a serem extraidos e transformados\n",
    "\n",
    "#At_atracacao_fato = [\"IDAtracacao\" , \" Tipo de Navegação da Atracação\" , \" CDTUP\" , \" Nacionalidade do Armador\" , \n",
    "#                     \" IDBerco\" , \" FlagMCOperacaoAtracacao\" , \" Berço Terminal\" , \"Porto Atracação\" , \" Município\" , \n",
    "#                     \" Apelido Instalação Portuária\" , \" UF\" , \" Complexo Portuário\" , \" SGUF\" , \n",
    "#                     \" Tipo da Autoridade Portuária\" , \"Região Geográfica\" , \" Data Atracação\" , \" Nº da Capitania\" , \n",
    "#                     \" Data Chegada\" , \" Nº do IMO\" , \" Data Desatracação\" , \" TEsperaAtracacao\" , \"Data Início Operação\",\n",
    "#                     \" TEsperaInicioOp\" , \" Data Término Operação\" , \" TOperacao\" , \" Ano da data de início da operação\" ,\n",
    "#                     \" TEsperaDesatracacao\" , \"Mês da data de início da operação\" , \" TAtracado\" , \" Tipo de Operação\" , \n",
    "#                     \" TEstadia\"]\n",
    "##############################\n",
    "# carga_fato = [IDCarga\" , \" FlagTransporteViaInterioir\" , \" IDAtracacao\" , \" Percurso Transporte em vias Interiores\" , \n",
    "#                \" Origem Percurso Transporte Interiores\" , \"Destino\" , \" STNaturezaCarga\" , \n",
    "#                \" CDMercadoria (Para carga conteinerizadainformar código das mercadorias dentrodo contêiner.)\" , \" STSH2\" , \n",
    "#                \" Tipo Operação da Carga\" , \" STSH4\" , \" Carga Geral Acondicionamento\" , \" Natureza da Carga\" ,\n",
    "#                \" ConteinerEstado\" , \" Sentido\" , \" Tipo Navegação\" , \" TEU\" , \"FlagAutorizacao\" , \" QTCarga\" , \n",
    "#                \" FlagCabotagem\" , \" VLPesoCargaBruta\" , \" FlagCabotagemMovimentacao\" , \n",
    "#                \" Ano da data de início da operação da atracação\" , \" FlagConteinerTamanho\" , \n",
    "#                \" Mês da data de início da operação da atracação\" , \" FlagLongoCurso\" , \" Porto Atracação\" , \n",
    "#                \" FlagMCOperacaoCarga\" , \" SGUF\" , \" FlagOffshore\" , \n",
    "#                \"  Peso líquido da carga (Carga não conteinerizada = Peso bruto e Carga conteinerizada = Peso sem contêiner)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(NacionalidadeEmbarcacao='Não Informado', Ano Acordo Bilateral='2010', Total Acordo Bilateral='1906211,1270003', AcordoTipoNavegacao='Longo Curso', País='ARGENTINA', FlagEmbarqueDesembarque='1')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_ab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NacionalidadeEmbarcacao',\n",
       " 'Ano Acordo Bilateral',\n",
       " 'Total Acordo Bilateral',\n",
       " 'AcordoTipoNavegacao',\n",
       " 'País',\n",
       " 'FlagEmbarqueDesembarque']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_ab.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDAtracacao='3060', CDTUP='BR', IDBerco=None, Berço=None, Porto Atracação='n/a', Apelido Instalação Portuária='n/a', Complexo Portuário='Não Classificado', Tipo da Autoridade Portuária='Porto Público', Data Atracação=None, Data Chegada=None, Data Desatracação='01/08/2020 00:00:00', Data Início Operação=None, Data Término Operação=None, Ano='2020', Mes='ago', Tipo de Operação=None, Tipo de Navegação da Atracação=None, Nacionalidade do Armador=None, FlagMCOperacaoAtracacao='1', Terminal=None, Município=None, UF=None, SGUF=None, Região Geográfica=None, Nº da Capitania=None, Nº do IMO=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDAtracacao',\n",
       " 'CDTUP',\n",
       " 'IDBerco',\n",
       " 'Berço',\n",
       " 'Porto Atracação',\n",
       " 'Apelido Instalação Portuária',\n",
       " 'Complexo Portuário',\n",
       " 'Tipo da Autoridade Portuária',\n",
       " 'Data Atracação',\n",
       " 'Data Chegada',\n",
       " 'Data Desatracação',\n",
       " 'Data Início Operação',\n",
       " 'Data Término Operação',\n",
       " 'Ano',\n",
       " 'Mes',\n",
       " 'Tipo de Operação',\n",
       " 'Tipo de Navegação da Atracação',\n",
       " 'Nacionalidade do Armador',\n",
       " 'FlagMCOperacaoAtracacao',\n",
       " 'Terminal',\n",
       " 'Município',\n",
       " 'UF',\n",
       " 'SGUF',\n",
       " 'Região Geográfica',\n",
       " 'Nº da Capitania',\n",
       " 'Nº do IMO']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDCarga='25376365', IDAtracacao='1048499', Origem='BR200', Destino='BRBEL', CDMercadoria='2710', Tipo Operação da Carga='Interior', Carga Geral Acondicionamento=None, ConteinerEstado=None, Tipo Navegação='Interior', FlagAutorizacao='S', FlagCabotagem='0', FlagCabotagemMovimentacao='0', FlagConteinerTamanho=None, FlagLongoCurso='0', FlagMCOperacaoCarga='1', FlagOffshore='0', FlagTransporteViaInterioir='1', Percurso Transporte em vias Interiores='Interior de percurso não identificado', Percurso Transporte Interiores='Navegação Interior', STNaturezaCarga='Exclusivo', STSH2='Exclusivo', STSH4='Exclusivo', Natureza da Carga='Granel Líquido e Gasoso', Sentido='Desembarcados', TEU='0', QTCarga='0', VLPesoCargaBruta='1005,32')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga',\n",
       " 'IDAtracacao',\n",
       " 'Origem',\n",
       " 'Destino',\n",
       " 'CDMercadoria',\n",
       " 'Tipo Operação da Carga',\n",
       " 'Carga Geral Acondicionamento',\n",
       " 'ConteinerEstado',\n",
       " 'Tipo Navegação',\n",
       " 'FlagAutorizacao',\n",
       " 'FlagCabotagem',\n",
       " 'FlagCabotagemMovimentacao',\n",
       " 'FlagConteinerTamanho',\n",
       " 'FlagLongoCurso',\n",
       " 'FlagMCOperacaoCarga',\n",
       " 'FlagOffshore',\n",
       " 'FlagTransporteViaInterioir',\n",
       " 'Percurso Transporte em vias Interiores',\n",
       " 'Percurso Transporte Interiores',\n",
       " 'STNaturezaCarga',\n",
       " 'STSH2',\n",
       " 'STSH4',\n",
       " 'Natureza da Carga',\n",
       " 'Sentido',\n",
       " 'TEU',\n",
       " 'QTCarga',\n",
       " 'VLPesoCargaBruta']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDCarga='25506812', CDMercadoriaConteinerizada='0101', VLPesoCargaConteinerizada='17,8')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_contz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga', 'CDMercadoriaConteinerizada', 'VLPesoCargaConteinerizada']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_contz.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDCarga='25376365', Região Hidrográfica=None, ValorMovimentado='1005,32')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga', 'Região Hidrográfica', 'ValorMovimentado']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_reg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDAtracacao='1117343', TEsperaAtracacao='0,48333333333721', TEsperaInicioOp='0,18333333340706', TOperacao='39,249999999942', TEsperaDesatracacao='0,25000000011642', TAtracado='39,683333333465', TEstadia='40,166666666802')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_temp_atr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDAtracacao',\n",
       " 'TEsperaAtracacao',\n",
       " 'TEsperaInicioOp',\n",
       " 'TOperacao',\n",
       " 'TEsperaDesatracacao',\n",
       " 'TAtracado',\n",
       " 'TEstadia']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_temp_atr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "ano = [\"2018\",\"2019\",\"2020\"]\n",
    "for i in ano:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(IDCarga='25376365', Região Hidrográfica=None, ValorMovimentado='1005,32')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga',\n",
       " 'IDAtracacao',\n",
       " 'Origem',\n",
       " 'Destino',\n",
       " 'CDMercadoria',\n",
       " 'Tipo Operação da Carga',\n",
       " 'Carga Geral Acondicionamento',\n",
       " 'ConteinerEstado',\n",
       " 'Tipo Navegação',\n",
       " 'FlagAutorizacao',\n",
       " 'FlagCabotagem',\n",
       " 'FlagCabotagemMovimentacao',\n",
       " 'FlagConteinerTamanho',\n",
       " 'FlagLongoCurso',\n",
       " 'FlagMCOperacaoCarga',\n",
       " 'FlagOffshore',\n",
       " 'FlagTransporteViaInterioir',\n",
       " 'Percurso Transporte em vias Interiores',\n",
       " 'Percurso Transporte Interiores',\n",
       " 'STNaturezaCarga',\n",
       " 'STSH2',\n",
       " 'STSH4',\n",
       " 'Natureza da Carga',\n",
       " 'Sentido',\n",
       " 'TEU',\n",
       " 'QTCarga',\n",
       " 'VLPesoCargaBruta']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDAtracacao',\n",
       " 'CDTUP',\n",
       " 'IDBerco',\n",
       " 'Berço',\n",
       " 'Porto Atracação',\n",
       " 'Apelido Instalação Portuária',\n",
       " 'Complexo Portuário',\n",
       " 'Tipo da Autoridade Portuária',\n",
       " 'Data Atracação',\n",
       " 'Data Chegada',\n",
       " 'Data Desatracação',\n",
       " 'Data Início Operação',\n",
       " 'Data Término Operação',\n",
       " 'Ano',\n",
       " 'Mes',\n",
       " 'Tipo de Operação',\n",
       " 'Tipo de Navegação da Atracação',\n",
       " 'Nacionalidade do Armador',\n",
       " 'FlagMCOperacaoAtracacao',\n",
       " 'Terminal',\n",
       " 'Município',\n",
       " 'UF',\n",
       " 'SGUF',\n",
       " 'Região Geográfica',\n",
       " 'Nº da Capitania',\n",
       " 'Nº do IMO']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 p style='text-align: center;'> Tabela atracacao_fato <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Tabela atracacao_fato#######\n",
    "#Criação da tabeça atracacao_fato\n",
    "# Campo comun entre as duas tabelas (tb_atr.IDAtracacao == tb_temp_atr.IDAtracacao)\n",
    "# Inner - Combina os valores comuns dos mesmos campos nas duas tabelas (Intersecção)\n",
    "\n",
    "# criar uma sessão usanda para extração da tabela atracacao_fato\n",
    "spark = SparkSession.builder.appName(\"ANTAQ.atracacao_fato\").getOrCreate()\n",
    "\n",
    "atracacao_fato = tb_atr.join(tb_temp_atr, tb_atr.IDAtracacao == tb_temp_atr.IDAtracacao, \"inner\" )\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229574"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227119"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_temp_atr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227119"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atracacao_fato.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDAtracacao',\n",
       " 'CDTUP',\n",
       " 'IDBerco',\n",
       " 'Berço',\n",
       " 'Porto Atracação',\n",
       " 'Apelido Instalação Portuária',\n",
       " 'Complexo Portuário',\n",
       " 'Tipo da Autoridade Portuária',\n",
       " 'Data Atracação',\n",
       " 'Data Chegada',\n",
       " 'Data Desatracação',\n",
       " 'Data Início Operação',\n",
       " 'Data Término Operação',\n",
       " 'Ano',\n",
       " 'Mes',\n",
       " 'Tipo de Operação',\n",
       " 'Tipo de Navegação da Atracação',\n",
       " 'Nacionalidade do Armador',\n",
       " 'FlagMCOperacaoAtracacao',\n",
       " 'Terminal',\n",
       " 'Município',\n",
       " 'UF',\n",
       " 'SGUF',\n",
       " 'Região Geográfica',\n",
       " 'Nº da Capitania',\n",
       " 'Nº do IMO',\n",
       " 'IDAtracacao',\n",
       " 'TEsperaAtracacao',\n",
       " 'TEsperaInicioOp',\n",
       " 'TOperacao',\n",
       " 'TEsperaDesatracacao',\n",
       " 'TAtracado',\n",
       " 'TEstadia']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atracacao_fato.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da Tabela 'atracacao_fato.csv' no diretório atual da execução do Jupyter / se desejar outro Path basta colocar na string\n",
    "atracacao_fato.toPandas().to_csv('atracacao_fato.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga',\n",
       " 'IDAtracacao',\n",
       " 'Origem',\n",
       " 'Destino',\n",
       " 'CDMercadoria',\n",
       " 'Tipo Operação da Carga',\n",
       " 'Carga Geral Acondicionamento',\n",
       " 'ConteinerEstado',\n",
       " 'Tipo Navegação',\n",
       " 'FlagAutorizacao',\n",
       " 'FlagCabotagem',\n",
       " 'FlagCabotagemMovimentacao',\n",
       " 'FlagConteinerTamanho',\n",
       " 'FlagLongoCurso',\n",
       " 'FlagMCOperacaoCarga',\n",
       " 'FlagOffshore',\n",
       " 'FlagTransporteViaInterioir',\n",
       " 'Percurso Transporte em vias Interiores',\n",
       " 'Percurso Transporte Interiores',\n",
       " 'STNaturezaCarga',\n",
       " 'STSH2',\n",
       " 'STSH4',\n",
       " 'Natureza da Carga',\n",
       " 'Sentido',\n",
       " 'TEU',\n",
       " 'QTCarga',\n",
       " 'VLPesoCargaBruta']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDAtracacao',\n",
       " 'CDTUP',\n",
       " 'IDBerco',\n",
       " 'Berço',\n",
       " 'Porto Atracação',\n",
       " 'Apelido Instalação Portuária',\n",
       " 'Complexo Portuário',\n",
       " 'Tipo da Autoridade Portuária',\n",
       " 'Data Atracação',\n",
       " 'Data Chegada',\n",
       " 'Data Desatracação',\n",
       " 'Data Início Operação',\n",
       " 'Data Término Operação',\n",
       " 'Ano',\n",
       " 'Mes',\n",
       " 'Tipo de Operação',\n",
       " 'Tipo de Navegação da Atracação',\n",
       " 'Nacionalidade do Armador',\n",
       " 'FlagMCOperacaoAtracacao',\n",
       " 'Terminal',\n",
       " 'Município',\n",
       " 'UF',\n",
       " 'SGUF',\n",
       " 'Região Geográfica',\n",
       " 'Nº da Capitania',\n",
       " 'Nº do IMO']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga', 'CDMercadoriaConteinerizada', 'VLPesoCargaConteinerizada']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_contz.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teste filtro\n",
    "tb_atr_filtro = tb_atr.select('IDAtracacao','Ano','Mes','Porto Atracação', 'SGUF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+---------------+----+\n",
      "|IDAtracacao| Ano|Mes|Porto Atracação|SGUF|\n",
      "+-----------+----+---+---------------+----+\n",
      "|       3060|2020|ago|            n/a|null|\n",
      "|       3061|2020|ago|            n/a|null|\n",
      "|       3073|2020|ago|            n/a|null|\n",
      "|       3067|2020|ago|            n/a|null|\n",
      "|       3059|2020|jul|            n/a|null|\n",
      "|       3114|2020|set|            n/a|null|\n",
      "|       3088|2020|ago|            n/a|null|\n",
      "|       3112|2020|set|            n/a|null|\n",
      "|       3062|2020|ago|            n/a|null|\n",
      "|       3115|2020|set|            n/a|null|\n",
      "|       3113|2020|set|            n/a|null|\n",
      "|       3066|2020|ago|            n/a|null|\n",
      "|       3052|2020|jul|            n/a|null|\n",
      "|       3049|2020|jul|            n/a|null|\n",
      "|       3057|2020|jul|            n/a|null|\n",
      "|       3056|2020|jul|            n/a|null|\n",
      "|       3197|2020|dez|            n/a|null|\n",
      "|       3050|2020|jul|            n/a|null|\n",
      "|       3063|2020|ago|            n/a|null|\n",
      "|       3065|2020|ago|            n/a|null|\n",
      "+-----------+----+---+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_atr_filtro.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste Carga coneinerizada para pegar o valor do conteinerizado\n",
    "tb_carga_contz_filtro = tb_carga_contz.select('IDCarga','VLPesoCargaConteinerizada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga', 'VLPesoCargaConteinerizada']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_contz_filtro.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste Carga para pegar o valor da carga bruta\n",
    "tb_carga_filtro = tb_carga.select('IDCarga','VLPesoCargaBruta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junção dos filtros com informações de cargas bruta e conteinrizada\n",
    "# tb_carga_contz_filtro.IDCarga == tb_carga_filtro.IDCarga, \"inner\" == ['IDCarga']\n",
    "tb_carga_liqui1 = tb_carga_filtro.join(tb_carga_contz_filtro, ['IDCarga'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga', 'VLPesoCargaBruta', 'VLPesoCargaConteinerizada']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_liqui1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_carga_liqui2 = tb_carga_liqui1.withColumn('Peso Liquido',tb_carga_liqui1['VLPesoCargaBruta'] + tb_carga_liqui1['VLPesoCargaBruta'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga', 'VLPesoCargaBruta', 'VLPesoCargaConteinerizada', 'Peso Liquido']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_liqui2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste para pegar a informação do peso liquido\n",
    "tb_carga_liqui = tb_carga_liqui2.select('IDCarga','Peso Liquido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga', 'Peso Liquido']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_carga_liqui.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teste\n",
    "#Procedimentos de junções finais\n",
    "# tb_carga.IDAtracacao == tb_atr_filtro.IDAtracacao, \"inner\" == ['IDAtracacao']\n",
    "carga_fato_tmp1 = tb_carga.join(tb_atr_filtro, ['IDAtracacao'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDAtracacao',\n",
       " 'IDCarga',\n",
       " 'Origem',\n",
       " 'Destino',\n",
       " 'CDMercadoria',\n",
       " 'Tipo Operação da Carga',\n",
       " 'Carga Geral Acondicionamento',\n",
       " 'ConteinerEstado',\n",
       " 'Tipo Navegação',\n",
       " 'FlagAutorizacao',\n",
       " 'FlagCabotagem',\n",
       " 'FlagCabotagemMovimentacao',\n",
       " 'FlagConteinerTamanho',\n",
       " 'FlagLongoCurso',\n",
       " 'FlagMCOperacaoCarga',\n",
       " 'FlagOffshore',\n",
       " 'FlagTransporteViaInterioir',\n",
       " 'Percurso Transporte em vias Interiores',\n",
       " 'Percurso Transporte Interiores',\n",
       " 'STNaturezaCarga',\n",
       " 'STSH2',\n",
       " 'STSH4',\n",
       " 'Natureza da Carga',\n",
       " 'Sentido',\n",
       " 'TEU',\n",
       " 'QTCarga',\n",
       " 'VLPesoCargaBruta',\n",
       " 'Ano',\n",
       " 'Mes',\n",
       " 'Porto Atracação',\n",
       " 'SGUF']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carga_fato_tmp1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste Final para colocar a informação do peso liquido na tabela carga_fato\n",
    "#carga_fato_tmp1.IDCarga == tb_carga_liqui.IDCarga, \"inner\" == ['IDCarga']\n",
    "carga_fato = carga_fato_tmp1.join(tb_carga_liqui, ['IDCarga'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IDCarga',\n",
       " 'IDAtracacao',\n",
       " 'Origem',\n",
       " 'Destino',\n",
       " 'CDMercadoria',\n",
       " 'Tipo Operação da Carga',\n",
       " 'Carga Geral Acondicionamento',\n",
       " 'ConteinerEstado',\n",
       " 'Tipo Navegação',\n",
       " 'FlagAutorizacao',\n",
       " 'FlagCabotagem',\n",
       " 'FlagCabotagemMovimentacao',\n",
       " 'FlagConteinerTamanho',\n",
       " 'FlagLongoCurso',\n",
       " 'FlagMCOperacaoCarga',\n",
       " 'FlagOffshore',\n",
       " 'FlagTransporteViaInterioir',\n",
       " 'Percurso Transporte em vias Interiores',\n",
       " 'Percurso Transporte Interiores',\n",
       " 'STNaturezaCarga',\n",
       " 'STSH2',\n",
       " 'STSH4',\n",
       " 'Natureza da Carga',\n",
       " 'Sentido',\n",
       " 'TEU',\n",
       " 'QTCarga',\n",
       " 'VLPesoCargaBruta',\n",
       " 'Ano',\n",
       " 'Mes',\n",
       " 'Porto Atracação',\n",
       " 'SGUF',\n",
       " 'Peso Liquido']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carga_fato.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da Tabela 'carga_fato.csv' no diretório atual da execução do Jupyter / se desejar outro Path basta colocar na string\n",
    "atracacao_fato.toPandas().to_csv('carga_fato.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'IDCarga' is ambiguous, could be: IDCarga, IDCarga.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-ce2a2de4038a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtb_carga_liqui1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb_carga_filtro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb_carga_contz_filtro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_carga_contz_filtro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIDCarga\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtb_carga_filtro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIDCarga\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inner\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtb_carga_liqui2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb_carga_liqui1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Peso Liquido'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtb_carga_liqui1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'VLPesoCargaBruta'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtb_carga_liqui1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'VLPesoCargaBruta'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtb_carga_liqui\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb_carga_liqui2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IDCarga'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Peso Liquido'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Procedimentos de junções finais\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1667\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m         \"\"\"\n\u001b[1;32m-> 1669\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1670\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Reference 'IDCarga' is ambiguous, could be: IDCarga, IDCarga."
     ]
    }
   ],
   "source": [
    "######## Tabela carga_fato####### Resumo das Operações\n",
    "#Criação da tabeça carga_fato\n",
    "# Campo comun entre as duas tabelas (tb_atr.IDAtracacao == tb_temp_atr.IDAtracacao)\n",
    "# Inner - Combina os valores comuns dos mesmos campos nas duas tabelas (Intersecção)\n",
    "\n",
    "# criar uma sessão usanda para extração da tabela carga_fato\n",
    "spark = SparkSession.builder.appName(\"ANTAQ.carga_fato\").getOrCreate()\n",
    "\n",
    "#carga_fato = tb_atr.join(tb_temp_atr, tb_atr.IDAtracacao == tb_temp_atr.IDAtracacao, \"inner\" )\n",
    "#Filtro da tabela de atracação e crga conteinerizada\n",
    "tb_atr_filtro = tb_atr.select('IDAtracacao','Ano','Mes','Porto Atracação', 'SGUF')\n",
    "tb_carga_contz_filtro = tb_carga_contz.select('IDCarga','VLPesoCargaConteinerizada')\n",
    "tb_carga_filtro = tb_carga.select('IDCarga','VLPesoCargaBruta')\n",
    "\n",
    "#Procedimento para pegar os valores de modo que : Carga não conteinerizada = Peso bruto e Carga \n",
    "#conteinerizada = Peso sem contêiner - \n",
    "tb_carga_liqui1 = tb_carga_filtro.join(tb_carga_contz_filtro, tb_carga_contz_filtro.IDCarga == tb_carga_filtro.IDCarga, \"inner\" )\n",
    "tb_carga_liqui2 = tb_carga_liqui1.withColumn('Peso Liquido',tb_carga_liqui1['VLPesoCargaBruta'] + tb_carga_liqui1['VLPesoCargaBruta']  )\n",
    "tb_carga_liqui = tb_carga_liqui2.select('IDCarga','Peso Liquido')\n",
    "\n",
    "#Procedimentos de junções finais\n",
    "# tb_carga.IDAtracacao == tb_atr_filtro.IDAtracacao, \"inner\" == ['IDAtracacao']\n",
    "carga_fato_tmp1 = tb_carga.join(tb_atr_filtro, ['IDAtracacao'] )\n",
    "\n",
    "#carga_fato_tmp1.IDCarga == tb_carga_liqui.IDCarga, \"inner\" == ['IDCarga']\n",
    "carga_fato = carga_fato_tmp1.join(tb_carga_liqui, ['IDCarga'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Ano: string, Mes: string, Porto Atracação: string, SGUF: string]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_atr.select('Ano','Mes','Porto Atracação', 'SGUF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-104-6cff80ae00fa>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-104-6cff80ae00fa>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    return tb_ab\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Automatizar a extração\n",
    "# ler o arquivo .csv/.txt com o cabeçalho e esquema para os anos 2018, 2019 e 2020\n",
    "\n",
    "import os.path\n",
    "# Carga da tabela de AcordosBilaterais\n",
    "def Carga_ANTAQ(tb_ab):\n",
    "    ano = [\"2018\",\"2019\",\"2020\"]\n",
    "    tb_ab = pyspark.sql.dataframe.DataFrame()\n",
    "    for i in ano:\n",
    "        # verifica se o arquivo existe no dietório\n",
    "        if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+i+\"/\"+i+\"AcordosBilaterais.txt\"):\n",
    "            tb_ab = tb_ab.union(spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+i+\"/\"+i+\"AcordosBilaterais.txt\", header=True, sep=';'))\n",
    "            #rdd = tb_ab.union(rdd)\n",
    "            #tb_ab = tb_ab.union(rdd)\n",
    "return tb_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-3cb0847bce95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtb_acbi_2018\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D:\\Aulas 2021.1\\SFIEC Seleção\\Dados\\2018\\2018AcordosBilaterais.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "tb_acbi_2018 = sc.(self, 'D:\\Aulas 2021.1\\SFIEC Seleção\\Dados\\2018\\2018AcordosBilaterais.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/D:/Aulas 2021.1/SFIEC Seleção/Dados88AcordosBilaterais.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5f9ae38761fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtb_acbi_2018\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1233\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m         \"\"\"\n\u001b[1;32m-> 1235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m         \"\"\"\n\u001b[1;32m-> 1224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1079\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark_3.1.1_bin_hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/D:/Aulas 2021.1/SFIEC Seleção/Dados88AcordosBilaterais.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "tb_acbi_2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automatizar a extração para 2020\n",
    "#antaq2020 = zipfile.ZipFile('http://web.antaq.gov.br/Sistemas/ArquivosAnuario/Arquivos/2020.zip');\n",
    "#w = zipfile.ZipFile('http://web.antaq.gov.br/Sistemas/ArquivosAnuario/Arquivos/2020.zip') as antaq2020;\n",
    "    ##print(*antaq2020.namelist(), sep=\"\\n\")\n",
    "   \n",
    "#with zipfile.ZipFile('http://web.antaq.gov.br/Sistemas/ArquivosAnuario/Arquivos/2019.zip') as antaq2019:\n",
    "#with zipfile.ZipFile('http://web.antaq.gov.br/Sistemas/ArquivosAnuario/Arquivos/2018.zip') as antaq2018:\n",
    "#with zipfile.ZipFile('http://web.antaq.gov.br/Sistemas/ArquivosAnuario/Arquivos/MetadadosFrota.zip') as meta_frota:\n",
    "#with zipfile.ZipFile('http://web.antaq.gov.br/Sistemas/ArquivosAnuario/Arquivos/MetadadosAfretamento.zip') as meta_afreta:\n",
    "#with zipfile.ZipFile('http://web.antaq.gov.br/Sistemas/ArquivosAnuario/Arquivos/MetadadosMovimentacao.zip') as meta_movi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "ano = [\"2018\",\"2019\",\"2020\"]\n",
    "for i in ano:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'jdf' and 'sql_ctx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-12e2a9249e74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mano\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"2018\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"2019\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"2020\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtb_ab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mano\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;31m# verifica se o arquivo existe no dietório\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;31m##if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+i+\"/\"+i+\"AcordosBilaterais.txt\"):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'jdf' and 'sql_ctx'"
     ]
    }
   ],
   "source": [
    "#Testes\n",
    "ano = [\"2018\",\"2019\",\"2020\"]\n",
    "tb_ab = pyspark.sql.dataframe.DataFrame()\n",
    "for i in ano:\n",
    "  # verifica se o arquivo existe no dietório\n",
    "  ##if os.path.exists(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+i+\"/\"+i+\"AcordosBilaterais.txt\"):\n",
    "    tb_ab = tb_ab.union(spark.read.csv(\"D:/Aulas 2021.1/SFIEC_Selecao/Dados/\"+i+\"/\"+i+\"AcordosBilaterais.txt\", header=True, sep=';'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
