{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação da DAG\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import, timedelta\n",
    "args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': airflow.utils.dates.days_ago(2)\n",
    "}\n",
    "dag = DAG(\n",
    "    dag_id='ANTAQ_ETL',\n",
    "    default_args=args,\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    dagrun_timeout=timedelta(minutes=60)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma Task usanda para extração da tabela Acordo Bilateral\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ.tb_ab\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "inicio = datetime.now()\n",
    "print(inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma Task usanda para extração da tabela Atracacao\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ.tb_atr\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "inicio = datetime.now()\n",
    "print(inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma Task usanda para extração da tabela Carga\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ.tb_carga\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "inicio = datetime.now()\n",
    "print(inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma Task usanda para extração da tabela Carga_Conteinerizada\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ.tb_carga_contz\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "inicio = datetime.now()\n",
    "print(inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma Task usanda para extração da tabela Carga_Regiao\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ.tb_carga_reg\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "inicio = datetime.now()\n",
    "print(inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma Task usanda para extração da tabela TemposAtracacao\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ.tb_temp_atr\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "inicio = datetime.now()\n",
    "print(inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma Task usanda para extração da tabela atracacao_fato\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ.atracacao_fato\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "inicio = datetime.now()\n",
    "print(inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar uma Task usanda para extração da tabela carga_fato\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ.carga_fato\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "inicio = datetime.now()\n",
    "print(inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Teste e Rascunhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imprime a data na saída padrão\n",
    "task1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag)\n",
    "# 2. Faz uma sleep de 5 segundos.\n",
    "# Dando errado tente em no máximo 3 vezes\n",
    "task2 = BashOperator(\n",
    "    task_id='sleep',\n",
    "    bash_command='sleep 5',\n",
    "    retries=3,\n",
    "    dag=dag)\n",
    "# 3. Salve a data em um arquivo texto\n",
    "task3 = BashOperator(\n",
    "    task_id='save_date',\n",
    "    bash_command='date > /tmp/date_output.txt',\n",
    "    retries=3,\n",
    "    dag=dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import airflow\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from pyspark.sql import SparkSession, functions\n",
    "\n",
    "def processo_etl_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ANTAQ\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/Financeiro\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.network.timeout\", 10000000) \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", 10000000) \\\n",
    "        .config(\"spark.storage.blockManagerSlaveTimeoutMs\", 10000000) \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .master(\"spark://192.168.0.1:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    inicio = datetime.now()\n",
    "    print(inicio)\n",
    "    # criação de data frame com extração de dados\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:sqlserver://127.0.0.1:1433;databaseName=Teste\") \\\n",
    "        .option(\"user\", 'Teste') \\\n",
    "        .option(\"password\", 'teste') \\\n",
    "        .option(\"numPartitions\", 100) \\\n",
    "        .option(\"partitionColumn\", \"Id\") \\\n",
    "        .option(\"lowerBound\", 1) \\\n",
    "        .option(\"upperBound\", 488777675) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"dbtable\", \"(select Id, DataVencimento AS Vencimento, TipoCod AS CodigoTipoDocumento, cast(recsld as FLOAT) AS Saldo from DocumentoPagar \\\n",
    "         where TipoCod in ('200','17') and RecPag = 'A') T\") \\\n",
    "        .load()\n",
    "    # agrupamento de dados e agregação de valores\n",
    "    group = df.select(\"CodigoTipoDocumento\", \"Vencimento\", \"Saldo\") \\\n",
    "        .groupby([\"CodigoTipoDocumento\", \"Vencimento\"]).agg(functions.sum(\"Saldo\").alias(\"Saldo\"))\n",
    "    # carregamento de dados para dentro da base mongo\n",
    "    group.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"database\", \"Financeiro\") \\\n",
    "        .option(\"collection\", \"Fact_DocumentoPagar\") \\\n",
    "        .save()\n",
    "    termino = datetime.now()\n",
    "    print(termino)\n",
    "    print(termino - inicio)\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'jozimar',\n",
    "    'start_date': datetime(2020, 11, 18),\n",
    "    'retries': 10,\n",
    "\t  'retry_delay': timedelta(hours=1)\n",
    "}\n",
    "with airflow.DAG('dag_teste_spark_documento_vencido_v01',\n",
    "                  default_args=default_args,\n",
    "                  schedule_interval='0 1 * * *') as dag:\n",
    "    task_elt_documento_pagar = PythonOperator(\n",
    "        task_id='elt_documento_pagar_spark',\n",
    "        python_callable=processo_etl_spark\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import airflow\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Elifranio',\n",
    "    'start_date': datetime(2020, 11, 18),\n",
    "    'retries': 10,\n",
    "\t  'retry_delay': timedelta(hours=1)\n",
    "}\n",
    "with airflow.DAG('dag_teste_ANTAQ',\n",
    "                  default_args=default_args,\n",
    "                  schedule_interval='0 1 * * *') as dag:\n",
    "    task_elt_documento_pagar = BashOperator(\n",
    "        task_id='elt_documento_pagar_spark',\n",
    "        bash_command=\"python ./dags/sparkjob.py\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
